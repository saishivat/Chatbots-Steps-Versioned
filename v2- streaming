# chatbot.py
# Import necessary modules
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.llms import Ollama
import streamlit as st
import time

# Define a prompt template for the chatbot
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant, your name is bla bla. Please respond to the questions."),
        ("user", "Question: {question}")
    ]
)

# Set up the Streamlit framework
input_text = st.text_input("Ask your question!")  # Create a text input field in the Streamlit app

# Initialize the Ollama model
llm = Ollama(model="qwen2:0.5b")

# Create a chain that combines the prompt and the Ollama model
chain = prompt | llm

# Simulate streaming the response
if input_text:
    # Create a placeholder for the response
    response_placeholder = st.empty()
    
    # Get the full response
    full_response = chain.invoke({"question": input_text})
    
    # Initialize accumulated response text
    accumulated_response = ""
    
    # Simulate streaming by displaying the response in chunks
    chunk_size = 5  # Adjust chunk size if needed
    for i in range(0, len(full_response), chunk_size):
        chunk = full_response[i:i + chunk_size]
        accumulated_response += chunk
        response_placeholder.write(accumulated_response)
        time.sleep(0.1)  # Adjust delay to simulate streaming speed
